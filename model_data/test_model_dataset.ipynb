{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c618653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "temp_posix_path = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "# set environment variables to limit cpu usage\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"  # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"  # export OPENBLAS_NUM_THREADS=4\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"6\"  # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"  # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\"  # export NUMEXPR_NUM_THREADS=6\n",
    "from model_definitions.def_net import NNmodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8860001",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspath = Path(\"dataset_fmnist_hyp_fix.pt\")\n",
    "ds = torch.load(dspath)\n",
    "pathlib.PosixPath = temp_posix_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cfbc774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trainset', 'valset', 'testset'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18374/18374 [00:08<00:00, 2086.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights test shape: torch.Size([18374, 2464])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds.keys())\n",
    "weights_test = ds[\"testset\"].__get_weights__()\n",
    "print(f\"Weights test shape: {weights_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e504b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_autoload(props, weights, verbose=False):\n",
    "    model_types = [\"CNN\", \"CNN2\", \"CNN3\", \"Resnet18\", \"MLP\"]\n",
    "    props[\"model::channels_in\"] = weights[next(iter(weights))].shape[1]\n",
    "    props[\"model::o_dim\"] = weights[next(reversed(weights))].shape[0]\n",
    "    props[\"optim::momentum\"] = 0.99\n",
    "    props[\"scheduler::steps_per_epoch\"] = 1000\n",
    "    for model_type in model_types:\n",
    "        try:\n",
    "            props[\"model::type\"] = model_type\n",
    "            model = NNmodule(config=props)\n",
    "            weights2load = {f\"model.{key}\": value for key, value in weights.items()}\n",
    "            model.load_state_dict(weights2load)\n",
    "            return props, model\n",
    "        except RuntimeError as e:\n",
    "            continue\n",
    "    if verbose:\n",
    "        print(f\"Could not load model. Props: {props}\")\n",
    "    return props, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8d1af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c843201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNetDataset(Dataset):\n",
    "    \"\"\"Custom TinyImageNet dataset that maps folder names to sequential IDs using wnids.txt\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load wnids.txt to get the mapping from folder names to sequential IDs\n",
    "        wnids_path = os.path.join(os.path.dirname(root_dir), \"wnids.txt\")\n",
    "        with open(wnids_path, \"r\") as f:\n",
    "            self.wnids = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        # Create mapping from folder name to sequential ID\n",
    "        self.class_to_idx = {wnid: idx for idx, wnid in enumerate(self.wnids)}\n",
    "        \n",
    "        # Collect all image paths and their labels\n",
    "        self.samples = []\n",
    "        \n",
    "        # Check if we\"re using test or train structure\n",
    "        if \"test\" in root_dir:\n",
    "            # Test structure: test/images/test_*.JPEG\n",
    "            image_dir = root_dir\n",
    "            if os.path.exists(image_dir):\n",
    "                for img_file in glob.glob(os.path.join(image_dir, \"*.JPEG\")):\n",
    "                    # For test images, we don\"t have labels, so use -1 or handle appropriately\n",
    "                    self.samples.append((img_file, -1))  # -1 indicates unknown label for test\n",
    "        else:\n",
    "            # Train structure: train/n02124075/images/*.JPEG\n",
    "            for wnid in self.wnids:\n",
    "                class_dir = os.path.join(root_dir, wnid, \"images\")\n",
    "                if os.path.exists(class_dir):\n",
    "                    label = self.class_to_idx[wnid]\n",
    "                    for img_file in glob.glob(os.path.join(class_dir, \"*.JPEG\")):\n",
    "                        self.samples.append((img_file, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d52e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_data(data_path, source_type=\"pt_file\"):\n",
    "    \"\"\"\n",
    "    Load model data from either .pt file or folder structure\n",
    "    \n",
    "    Args:\n",
    "        data_path: path to data source (.pt file or folder)\n",
    "        source_type: \"pt_file\" or \"folder\"\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with trainset/testset containing model data\n",
    "    \"\"\"\n",
    "    if source_type == \"pt_file\":\n",
    "        return torch.load(data_path)\n",
    "    elif source_type == \"folder\":\n",
    "        # Load from folder structure: folder/some_sample/checkpoint_000060/checkpoints + folder/some_sample/params.json\n",
    "        data = {\"trainset\": [], \"testset\": []}\n",
    "        \n",
    "        # Find all sample folders\n",
    "        sample_folders = glob.glob(os.path.join(data_path, \"*\"))\n",
    "        sample_folders = [f for f in sample_folders if os.path.isdir(f)]\n",
    "        \n",
    "        for sample_folder in sample_folders:\n",
    "            # Load params.json\n",
    "            params_path = os.path.join(sample_folder, \"params.json\")\n",
    "            acc_path = os.path.join(sample_folder, \"progress.csv\")\n",
    "\n",
    "            with open(params_path, \"r\") as f:\n",
    "                props = json.load(f)\n",
    "            \n",
    "            with open(acc_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                first_line = lines[0].strip().split(\",\")\n",
    "                last_line = lines[-1].strip().split(\",\")\n",
    "                if \"test_acc\" in first_line:\n",
    "                    props[\"test_acc\"] = float(last_line[first_line.index(\"test_acc\")])\n",
    "            \n",
    "            # Load weights from checkpoint_000060\n",
    "            checkpoint_path = os.path.join(sample_folder, \"checkpoint_000060\")\n",
    "            if not os.path.exists(checkpoint_path):\n",
    "                continue\n",
    "            \n",
    "            weights = torch.load(os.path.join(checkpoint_path, \"checkpoints\"))            \n",
    "            data[\"trainset\"].append((props, weights))\n",
    "        \n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported source_type: {source_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1bc426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_save(ds, subset, threshold=0.8, dataset=\"FashionMNIST\", sanity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Validate models by accuracy threshold and sanity check on dataset\n",
    "    \n",
    "    Args:\n",
    "        ds: dataset dictionary containing model data\n",
    "        subset: dataset subset name\n",
    "        threshold: minimum accuracy threshold\n",
    "        dataset: dataset to use for sanity check (\"MNIST\", \"CIFAR10\", \"FashionMNIST\", \"TinyImageNet\")\n",
    "        sanity_threshold: minimum accuracy on sanity dataset\n",
    "    \"\"\"\n",
    "    # Create dataset for sanity checking\n",
    "    if dataset == \"MNIST\":\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, \n",
    "                                                 download=True, transform=transform)\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False,\n",
    "                                                   download=True, transform=transform)\n",
    "    elif dataset == \"FashionMNIST\":\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "        test_dataset = torchvision.datasets.FashionMNIST(root=\"./data\", train=False,\n",
    "                                                        download=True, transform=transform)\n",
    "    elif dataset == \"TinyImageNet\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),  # TinyImageNet is 64x64\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        # Use custom TinyImageNet dataset with proper ID mapping\n",
    "        test_dataset = TinyImageNetDataset(\n",
    "            root_dir=\"./data/tiny-imagenet-200/train\",  # Use train set for validation\n",
    "            transform=transform\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset}\")\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    valid_models = []\n",
    "    pbar = tqdm(range(len(ds[subset])), total=len(ds[subset]), desc=\"Validating models\")\n",
    "    for model_n in pbar:\n",
    "        props, weights = ds[subset][model_n]\n",
    "\n",
    "        # Check accuracy threshold\n",
    "        acc_key = \"test_acc\"\n",
    "        if props.get(acc_key, 0) < threshold:\n",
    "            continue\n",
    "            \n",
    "        # Try to load model\n",
    "        props_loaded, model = model_autoload(props, weights)\n",
    "        if model is None:\n",
    "            pbar.set_description(f\"Model {model_n}: acc={props[acc_key]:.3f} - FAILED to load\")\n",
    "            continue\n",
    "        # Sanity check: evaluate model on dataset\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "                if batch_idx >= 10:  # Only test on first 10 batches for speed\n",
    "                    break\n",
    "                \n",
    "                # Skip samples with unknown labels (test set)\n",
    "                if dataset == \"TinyImageNet\":\n",
    "                    valid_mask = target != -1\n",
    "                    if valid_mask.sum() == 0:\n",
    "                        continue\n",
    "                    data = data[valid_mask]\n",
    "                    target = target[valid_mask]\n",
    "                \n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        sanity_accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        # Only include if passes sanity check\n",
    "        if sanity_accuracy >= sanity_threshold:\n",
    "            valid_models.append((props_loaded, model))\n",
    "            pbar.set_description(f\"Model {model_n}: acc={props[acc_key]:.3f}, sanity_acc={sanity_accuracy:.3f} - PASSED\")\n",
    "        else:\n",
    "            pbar.set_description(f\"Model {model_n}: acc={props[acc_key]:.3f}, sanity_acc={sanity_accuracy:.3f} - FAILED sanity check\")\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"Valid models: {len(valid_models)}/{len(ds[subset])}\")\n",
    "    return valid_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70eec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 82039: acc=0.828, sanity_acc=0.662 - PASSED: 100%|██████████| 82040/82040 [2:04:07<00:00, 11.02it/s]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid models: 23839/82040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 81367: acc=0.779 - FAILED to load: 100%|██████████| 81368/81368 [2:41:31<00:00,  8.40it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid models: 34745/81368\n",
      "Valid ResNet models: 116\n",
      "Valid CIFAR models: 12658\n",
      "Valid MNIST models: 23839\n",
      "Valid FashionMNIST models: 34745\n",
      "Total valid models: 71358\n"
     ]
    }
   ],
   "source": [
    "# save valid train models to .pt file\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ds_resnet = load_model_data(\"./data/tiny-imagenet_resnet18_kaiming_uniform_subset\", source_type=\"folder\")\n",
    "# valid_resnet = validate_and_save(\n",
    "#     ds_resnet,\n",
    "#     \"trainset\",\n",
    "#     threshold=0.5,\n",
    "#     dataset=\"TinyImageNet\",\n",
    "#     sanity_threshold=0.5\n",
    "# )\n",
    "# torch.save(valid_resnet, \"valid_tinyimagenet_resnet_models.pt\")\n",
    "\n",
    "# ds_cifar = load_model_data(\"./data/dataset_cifar_large_hyp_rand.pt\")\n",
    "# valid_cifar = validate_and_save(\n",
    "#     ds_cifar,\n",
    "#     \"trainset\",\n",
    "#     threshold=0.5,\n",
    "#     dataset=\"CIFAR10\",\n",
    "#     sanity_threshold=0.5\n",
    "# )\n",
    "# torch.save(valid_cifar, \"valid_cifar_models.pt\")\n",
    "\n",
    "ds_mnist = load_model_data(\"./data/dataset_mnist_hyp_rand.pt\")\n",
    "valid_mnist = validate_and_save(\n",
    "    ds_mnist,\n",
    "    \"trainset\",\n",
    "    threshold=0.5,\n",
    "    dataset=\"MNIST\",\n",
    "    sanity_threshold=0.5\n",
    ")\n",
    "torch.save(valid_mnist, \"valid_mnist_models.pt\")\n",
    "\n",
    "ds_fmnist = load_model_data(\"./data/dataset_fmnist_hyp_rand.pt\")\n",
    "valid_fmnist = validate_and_save(\n",
    "    ds_fmnist,\n",
    "    \"trainset\",\n",
    "    threshold=0.5,\n",
    "    dataset=\"FashionMNIST\",\n",
    "    sanity_threshold=0.5\n",
    ")\n",
    "torch.save(valid_fmnist, \"valid_fmnist_models.pt\")\n",
    "\n",
    "valid_resnet = torch.load(\"valid_tinyimagenet_resnet_models.pt\")\n",
    "valid_cifar = torch.load(\"valid_cifar_models.pt\")\n",
    "\n",
    "all_valid = valid_resnet + valid_cifar + valid_mnist + valid_fmnist\n",
    "print(f\"Valid ResNet models: {len(valid_resnet)}\")\n",
    "print(f\"Valid CIFAR models: {len(valid_cifar)}\")\n",
    "print(f\"Valid MNIST models: {len(valid_mnist)}\")\n",
    "print(f\"Valid FashionMNIST models: {len(valid_fmnist)}\")\n",
    "print(f\"Total valid models: {len(all_valid)}\")\n",
    "torch.save(all_valid, \"valid_train_models.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capsnets_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
